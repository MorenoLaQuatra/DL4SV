

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Transformers &#8212; Deep Learning for Speech and Vision</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '3_transformers';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Computer Vision Applications" href="4_vision_applications.html" />
    <link rel="prev" title="Convolutional Neural Networks" href="2_cnns.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/cover_2.png" class="logo__image only-light" alt="Deep Learning for Speech and Vision - Home"/>
    <script>document.write(`<img src="_static/cover_2.png" class="logo__image only-dark" alt="Deep Learning for Speech and Vision - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Deep Learning for Speech and Vision
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1_dl_intro.html">Introduction to Deep Learning</a></li>


<li class="toctree-l1"><a class="reference internal" href="2_cnns.html">Convolutional Neural Networks</a></li>




<li class="toctree-l1 current active"><a class="current reference internal" href="#">Transformers</a></li>


<li class="toctree-l1"><a class="reference internal" href="4_vision_applications.html">Computer Vision Applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="5_audio_speech_applications.html">Audio Processing Applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F3_transformers.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/3_transformers.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Transformers</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Transformers</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-overview">Architecture Overview</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder">Encoder</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder">Decoder</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder-decoder">Encoder-Decoder</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-components">Transformer Components</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-layer">Embedding Layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encoding">Positional Encoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-mechanism">Attention Mechanism</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="transformers">
<h1>Transformers<a class="headerlink" href="#transformers" title="Permalink to this heading">#</a></h1>
<figure class="align-default" id="cover">
<a class="reference internal image-reference" href="_images/cover_transformers.png"><img alt="cover" src="_images/cover_transformers.png" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 33 </span><span class="caption-text">Image generated using <a class="reference external" href="https://huggingface.co/spaces/mrfakename/OpenDalleV1.1-GPU-Demo">OpenDALL-E</a></span><a class="headerlink" href="#cover" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h2>
<p>In this chapter, we will cover the basics of transformers, a type of neural network architecture that has been initially developed for natural language processing (NLP) tasks but has since been used and adapted for other modalities such as images, audio, and video.</p>
<p>The transformer architecture is designed for modeling sequential data, such as text, audio, and video. It is based on the idea of self-attention, which is a mechanism that allows the network to learn the relationships between different elements of a sequence. For example, in the case of an audio sequence, the network can learn the relationships between different frames of the audio signal and leverage the correlations between them to perform a task such as speech recognition.</p>
<p>The original Transformer architecture was introduced in the paper <a class="reference external" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> <span id="id1">[<a class="reference internal" href="references.html#id13" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 2017.">VSP+17</a>]</span>
by Vaswani et al. in 2017. Since then, many variants of the original architecture have been proposed, and transformers have become the state-of-the-art architecture for many tasks. In this chapter, we will cover all the building blocks of the transformer architecture and show how they can be used for different tasks.</p>
<figure class="align-default" id="architecture">
<a class="reference internal image-reference" href="_images/architecture.webp"><img alt="architecture" src="_images/architecture.webp" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 34 </span><span class="caption-text">Transformer architecture</span><a class="headerlink" href="#architecture" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="architecture-overview">
<h1>Architecture Overview<a class="headerlink" href="#architecture-overview" title="Permalink to this heading">#</a></h1>
<p><a class="reference internal" href="#architecture"><span class="std std-numref">Fig. 34</span></a> shows the architecture of the transformer model. The model consists of an encoder and a decoder.</p>
<ul class="simple">
<li><p>The <strong>encoder</strong> is responsible for processing the input sequence and extracting relevant information.</p></li>
<li><p>The <strong>decoder</strong> is responsible for generating the output sequence based on the information extracted by the encoder.</p></li>
</ul>
<p>The encoder and decoder are composed of a stack of identical layers. Each layer consists of multiple components, including a multi-head self-attention mechanism and a feed-forward network. We will cover each of these components in detail in the following sections.</p>
<p>üí° Transformers are designed to model <strong>discrete</strong> sequences, such as words in text, genes in DNA, or tokens in a programming language. They are not designed to model <strong>continuous</strong> sequences, such as audio or video. However, transformers can be used to model continuous sequences by discretizing them with specific techniques.</p>
<p>A few concepts are important to understand before we dive into the details of the transformer architecture.</p>
<ul class="simple">
<li><p><strong>Pre-training and fine-tuning</strong> is a technique that consists of training a model on a large amount of unlabeled data. The model is then fine-tuned on a specific task using a small amount of labeled data. Pre-training is a common technique used in deep learning to improve the performance of a model on a specific task. BERT <span id="id2">[<a class="reference internal" href="references.html#id14" title="Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.">DCLT18</a>]</span>, Wav2Vec 2.0 <span id="id3">[<a class="reference internal" href="references.html#id15" title="Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. Wav2vec 2.0: a framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:12449‚Äì12460, 2020.">BZMA20</a>]</span>, and ViT <span id="id4">[<a class="reference internal" href="references.html#id16" title="Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, and others. An image is worth 16x16 words: transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.">DBK+20</a>]</span> are examples of models that have been pre-trained on large amounts of data and fine-tuned on specific tasks.</p></li>
<li><p><strong>Discretization</strong> is used to convert continuous sequences into discrete sequences. For example, an audio signal is a continuous sequence, to convert it into a discrete sequence, we can split it into frames and define a <em>vocabulary</em> of frames. Once discretized, we can use <em>classification-like</em> approaches to train a transformer model on the audio sequence. Wav2Vec 2.0 <span id="id5">[<a class="reference internal" href="references.html#id15" title="Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. Wav2vec 2.0: a framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:12449‚Äì12460, 2020.">BZMA20</a>]</span> is an example of a model that uses discretization to train a transformer model on audio sequences.</p></li>
<li><p><strong>Positional encoding</strong> is a technique that consists of injecting information about the position of each element of a sequence into the model. We will see later that <em>attention</em> is a mechanism that allows the model to learn the relationships between the different elements of a sequence but it does not take into account the position of each element. Positional encoding is used to inject this information into the model.</p></li>
<li><p><strong>Encoder models</strong> are transformer models that only have an encoder. They are used to extract features from a sequence. BERT <span id="id6">[<a class="reference internal" href="references.html#id14" title="Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.">DCLT18</a>]</span> and ViT <span id="id7">[<a class="reference internal" href="references.html#id16" title="Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, and others. An image is worth 16x16 words: transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.">DBK+20</a>]</span> are examples of encoder models.</p></li>
<li><p><strong>Decoder models</strong> are transformer models that only have a decoder. They are used to generate a sequence based on a set of features. GPT-2 <span id="id8">[<a class="reference internal" href="references.html#id20" title="Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, and others. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.">RWC+19</a>]</span> and VioLA <span id="id9">[<a class="reference internal" href="references.html#id17" title="Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shujie Liu, Yashesh Gaur, Zhuo Chen, Jinyu Li, and Furu Wei. Viola: unified codec language models for speech recognition, synthesis, and translation. arXiv preprint arXiv:2305.16107, 2023.">WZZ+23</a>]</span> are examples of decoder models.</p></li>
<li><p><strong>Sequence-to-sequence models</strong> are transformer models that have both an encoder and a decoder. They are used to generate a sequence based on another sequence. BART <span id="id10">[]</span> and Whisper <span id="id11">[<a class="reference internal" href="references.html#id18" title="Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, 28492‚Äì28518. PMLR, 2023.">RKX+23</a>]</span> are examples of sequence-to-sequence models.</p></li>
</ul>
<p>Those concepts will be used throughout this chapter to describe the different transformer models.</p>
<section id="encoder">
<h2>Encoder<a class="headerlink" href="#encoder" title="Permalink to this heading">#</a></h2>
<p>The encoder is responsible for processing the input sequence and extracting relevant information. The goal is to train a neural network that can leverage the correlations between the different elements of the input sequence to perform <strong>discriminative</strong> tasks such as classification, regression, or sequence labeling.</p>
<p>The input of the encoder is a sequence of elements. For example, in the case of text, the input sequence is a sequence of words. In the case of audio, the input sequence is a sequence of frames. In the case of images, the input sequence is a sequence of patches. The sequence is first converted into a sequence of <em>vector embeddings</em> that are then processed by the encoder layers.</p>
<figure class="align-default" id="id12">
<a class="reference internal image-reference" href="_images/encoder_with_tensors_2.png"><img alt="encoder" src="_images/encoder_with_tensors_2.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 35 </span><span class="caption-text">Encoder Layer architecture. Image source <a class="reference external" href="http://jalammar.github.io/illustrated-transformer/">illustrated-transformer</a></span><a class="headerlink" href="#id12" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#id12"><span class="std std-numref">Fig. 35</span></a> shows the architecture of an encoder layer. The input sequence is first converted into a sequence of vector embeddings <span class="math notranslate nohighlight">\(X = \{x_1, x_2, ..., x_n\}\)</span> using an embedding layer. The embeddings are then processed by the self-attention layer and then pass through a feed-forward network. The output of the feed-forward network is then added to the input embeddings to produce the output embeddings <span class="math notranslate nohighlight">\(Y = \{y_1, y_2, ..., y_n\}\)</span>.</p>
<p>The encoder is composed of a stack of identical layers, all similar to the one shown in <a class="reference internal" href="#id12"><span class="std std-numref">Fig. 35</span></a>. The output of the encoder is the output embeddings <span class="math notranslate nohighlight">\(Y\)</span> of the last layer.</p>
</section>
<section id="decoder">
<h2>Decoder<a class="headerlink" href="#decoder" title="Permalink to this heading">#</a></h2>
<p>The decoder is responsible for generating the output sequence based on the information extracted by the encoder. The goal is to train a neural network that can leverage the correlations between the different elements of the input sequence to perform <strong>generative</strong> tasks such as text generation, image generation, or speech synthesis.</p>
<p>The input of the decoder is a sequence of elements. For example, in the case of audio, the input sequence is a sequence of frames. The sequence is first converted into a sequence of <em>vector embeddings</em> that are then processed by the decoder layers.</p>
<figure class="align-default" id="id13">
<a class="reference internal image-reference" href="_images/transformer-decoder-intro.png"><img alt="decoder" src="_images/transformer-decoder-intro.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 36 </span><span class="caption-text">Decoder Layer architecture. Image source <a class="reference external" href="https://jalammar.github.io/illustrated-gpt2/">illustrated-gpt-2</a></span><a class="headerlink" href="#id13" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The <em>masked self-attention</em> layer is similar to the self-attention layer of the encoder. The only difference is that the masked self-attention layer is masked to prevent the decoder from ‚Äúseeing‚Äù the future elements of the sequence. The output is then processed by a feed-forward network. The output of the feed-forward network is then added to the input embeddings to produce the output embeddings <span class="math notranslate nohighlight">\(Y = \{y_1, y_2, ..., y_n\}\)</span>.</p>
<p>When training a decoder-only model, the output embeddings <span class="math notranslate nohighlight">\(Y\)</span> at each position <span class="math notranslate nohighlight">\(i\)</span> are used to predict the next element of the sequence <span class="math notranslate nohighlight">\(y_{i+1}\)</span>.</p>
<p>üí° In contrast with RNNs, the training of the decoder is <strong>autoregressive</strong>. This means that the model is trained to predict the next element of the sequence based on the previous elements of the sequence. While for RNNs we need to recursively feed the output of the model back as input, for transformers we can compute the output of the model in parallel for all the elements of the sequence.</p>
<p>üí° During <strong>inference</strong>, the decoder is used to generate the output sequence. However, the target is not available during inference. Instead, the output of the decoder at each position <span class="math notranslate nohighlight">\(i\)</span> is used as input for the next position <span class="math notranslate nohighlight">\(i+1\)</span>. This process is repeated until a special token is generated or a maximum number of steps is reached. This is one of the reason why, the <strong>inference</strong> on transformers is slower than the <strong>training</strong>.</p>
</section>
<section id="encoder-decoder">
<h2>Encoder-Decoder<a class="headerlink" href="#encoder-decoder" title="Permalink to this heading">#</a></h2>
<p>If we combine the encoder and decoder, we get a sequence-to-sequence model. The encoder is used to extract features from the input sequence and the decoder is used to generate the output sequence based (or conditioned) on the extracted features.
One example of sequence-to-sequence model is a music style transfer model. The input sequence may be a song in a specific style and the output sequence may be the same song in another style. The encoder is used to extract features from the input song and the decoder is used to generate the output song based on the extracted features.</p>
<figure class="align-default" id="id14">
<a class="reference internal image-reference" href="_images/encoder_decoder.png"><img alt="encoder_decoder" src="_images/encoder_decoder.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 37 </span><span class="caption-text">Encoder-Decoder architecture. Image source <a class="reference external" href="http://jalammar.github.io/illustrated-transformer/">illustrated-transformer</a></span><a class="headerlink" href="#id14" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#id14"><span class="std std-numref">Fig. 37</span></a> shows the architecture of an encoder-decoder model. The input sequence is first converted into a sequence of vector embeddings <span class="math notranslate nohighlight">\(X = \{x_1, x_2, ..., x_n\}\)</span> using an embedding layer. The embeddings are then processed by the encoder layers. The output of the encoder is the output embeddings <span class="math notranslate nohighlight">\(Y = \{y_1, y_2, ..., y_n\}\)</span> of the last layer. The output embeddings are then processed by the decoder layers. Here we have an <strong>additional</strong> attention layer that allows the decoder to combine the output embeddings of the encoder with the output embeddings of the decoder. The output of the decoder is the output embeddings <span class="math notranslate nohighlight">\(Z = \{z_1, z_2, ..., z_n\}\)</span> of the last layer.</p>
<p>üñäÔ∏è The encoder-decoder attention is usually referred to as the <strong>cross-attention</strong> layer. The self-attention layer in the encoder is usually referred to as the <strong>self-attention</strong> layer. The self-attention layer in the decoder is usually referred to as the <strong>masked self-attention</strong> layer because it is masked to prevent the decoder from ‚Äúseeing‚Äù the future elements of the sequence. All these layers, however, perform the same operation that we will describe in the following sections.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="transformer-components">
<h1>Transformer Components<a class="headerlink" href="#transformer-components" title="Permalink to this heading">#</a></h1>
<figure class="align-default" id="architecture-2">
<a class="reference internal image-reference" href="_images/architecture.webp"><img alt="architecture_2" src="_images/architecture.webp" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 38 </span><span class="caption-text">Encoder-decoder Transformer architecture.</span><a class="headerlink" href="#architecture-2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>We will describe the different components of the transformer architecture from <strong>bottom to top</strong>. We will follow the <a class="reference internal" href="#architecture"><span class="std std-numref">Fig. 34</span></a> and start with the embedding layer, then the positional encoding, the self-attention layer, and so on.</p>
<section id="embedding-layer">
<h2>Embedding Layer<a class="headerlink" href="#embedding-layer" title="Permalink to this heading">#</a></h2>
<p>When training a transformer model, the input sequence is first converted into a sequence of <em>vector embeddings</em>. Those vector embeddings are created using an embedding layer. The embedding layer is a simple linear layer that maps each element of the input sequence to a vector of a specific size. The size of the vector is called the <em>embedding size</em> and is a power of 2, usually between 128 and 1024. The embedding size is a <strong>hyperparameter</strong> of the model.</p>
<p>We can see the embedding layer as a lookup table that maps each element of the input sequence to a vector of a specific size. The embedding layer is initialized randomly and is trained through backpropagation. The embedding layer is usually the first layer of the encoder and the decoder.</p>
<figure class="align-default" id="lookup-table">
<a class="reference internal image-reference" href="_images/lookup_table.gif"><img alt="lookup_table" src="_images/lookup_table.gif" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 39 </span><span class="caption-text">Embedding layer as a lookup table. Image source <a class="reference external" href="https://lena-voita.github.io/nlp_course/word_embeddings.html">lena-voita</a></span><a class="headerlink" href="#lookup-table" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#lookup-table"><span class="std std-numref">Fig. 39</span></a> shows an example of an embedding layer in the context of NLP (it is simpler to visualize in this context). The embedding layer is a lookup table that maps each word of the input sequence to a vector of a specific size.</p>
</section>
<section id="positional-encoding">
<h2>Positional Encoding<a class="headerlink" href="#positional-encoding" title="Permalink to this heading">#</a></h2>
<p>After the embedding layer, the input sequence is converted into a sequence of vector embeddings. As we can see later, the attention mechanism, at the core of the transformer architecture, does not take into account the position of each element of the sequence. To inject this information into the model, we use a technique called <em>positional encoding</em>.</p>
<p>There are different implementations of positional encoding. The traditional implementation is based on sinusoidal functions. For each position <span class="math notranslate nohighlight">\(i\)</span> of the input sequence, we compute a vector <span class="math notranslate nohighlight">\(PE_i\)</span> of the same size as the embeddings. The vector <span class="math notranslate nohighlight">\(PE_i\)</span> is then added to the embeddings <span class="math notranslate nohighlight">\(x_i\)</span> to produce the final embeddings <span class="math notranslate nohighlight">\(x_i + PE_i\)</span>.</p>
<p><strong>How can we compute the vector <span class="math notranslate nohighlight">\(PE_i\)</span>?</strong> The vector <span class="math notranslate nohighlight">\(PE_i\)</span> is computed using a combination of sinusoidal functions. We define a set of frequencies <span class="math notranslate nohighlight">\(f\)</span> and compute the vector <span class="math notranslate nohighlight">\(PE_i\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}PE_i = \begin{bmatrix} sin(f_1 \times i) \\ cos(f_1 \times i) \\ sin(f_2 \times i) \\ cos(f_2 \times i) \\ \vdots \\ sin(f_{d/2} \times i) \\ cos(f_{d/2} \times i) \end{bmatrix}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(d\)</span> is the size of the embeddings. The frequencies <span class="math notranslate nohighlight">\(f\)</span> are computed as follows:</p>
<div class="math notranslate nohighlight">
\[f_i = \frac{1}{10000^{2i/d}}\]</div>
<p>The frequencies <span class="math notranslate nohighlight">\(f\)</span> are computed using a geometric progression. The first frequency is <span class="math notranslate nohighlight">\(f_1 = 1/10000^{2 \times 1/d}\)</span>, the second frequency is <span class="math notranslate nohighlight">\(f_2 = 1/10000^{2 \times 2/d}\)</span>, and so on. The frequencies are then used to compute the vector <span class="math notranslate nohighlight">\(PE_i\)</span>. <span class="math notranslate nohighlight">\(d\)</span> is the size of the embeddings. The vector <span class="math notranslate nohighlight">\(PE_i\)</span> is then added to the embeddings <span class="math notranslate nohighlight">\(x_i\)</span> to produce the vector <span class="math notranslate nohighlight">\(x_i + PE_i\)</span> that will be the input of the network.</p>
<figure class="align-default" id="id15">
<a class="reference internal image-reference" href="_images/transformer_positional_encoding_example.png"><img alt="positional_encoding" src="_images/transformer_positional_encoding_example.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 40 </span><span class="caption-text">Positional encoding example. Image source <a class="reference external" href="http://jalammar.github.io/illustrated-transformer/">illustrated-transformer</a></span><a class="headerlink" href="#id15" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#id15"><span class="std std-numref">Fig. 40</span></a> shows an example of positional encoding (again in the context of NLP). Each element in a <span class="math notranslate nohighlight">\(PE_i\)</span> vector is computed using a sinusoidal function.</p>
<p>üí° The idea behind the use of sinusoidal functions is to allow the model to be able to encode regularities in the position of the elements of the sequence. Different frequencies may have similar values with different regularities. In a complete data-driven approach, the model would learn the regularities according to the patterns found in the data.</p>
</section>
<section id="attention-mechanism">
<h2>Attention Mechanism<a class="headerlink" href="#attention-mechanism" title="Permalink to this heading">#</a></h2>
<p>At this point of the chapter, we have converted the input sequence into a sequence of vector embeddings. The next step is to process the embeddings using the attention mechanism. The attention mechanism is the core of the transformer architecture. It is used to learn the relationships between the different elements of the sequence.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="2_cnns.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Convolutional Neural Networks</p>
      </div>
    </a>
    <a class="right-next"
       href="4_vision_applications.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Computer Vision Applications</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Transformers</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-overview">Architecture Overview</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder">Encoder</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder">Decoder</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder-decoder">Encoder-Decoder</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-components">Transformer Components</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-layer">Embedding Layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encoding">Positional Encoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-mechanism">Attention Mechanism</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Moreno La Quatra
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>